<?xml version="1.0" encoding="UTF-8"?>
<!--Generated by Site-Server v@build.version@ (http://www.squarespace.com) on Tue, 25 Nov 2025 21:35:33 GMT
--><rss xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:wfw="http://wellformedweb.org/CommentAPI/" xmlns:itunes="http://www.itunes.com/dtds/podcast-1.0.dtd" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:media="http://www.rssboard.org/media-rss" version="2.0"><channel><title>Journal - Earth Innovation Hub</title><link>https://earthinnovationhub.org/journal/</link><lastBuildDate>Sat, 17 May 2025 17:35:15 +0000</lastBuildDate><language>en-US</language><generator>Site-Server v@build.version@ (http://www.squarespace.com)</generator><description><![CDATA[]]></description><item><title>From Meshes to Meaning: AI-Assisted Digital Twin Synthesis</title><dc:creator>Jnaneshwar Das</dc:creator><pubDate>Thu, 15 May 2025 05:29:19 +0000</pubDate><link>https://earthinnovationhub.org/journal/from-meshes-to-meaning-ai-assisted-digital-twin-synthesis-via-model-context-protocols</link><guid isPermaLink="false">6736ca8aad6dbf3dff111519:68170c8f3bab4c2b72741231:68257b5847e78e6ffdf45349</guid><description><![CDATA[<p class="">STL files might be ubiquitous in the world of 3D modeling, but they're not ideal for precise CAD workflows. What they <em>do</em> offer, though, is a convenient on-ramp for sculpting and rapid ideation—especially when working with tools like <em>Blender</em>, where artistic flexibility meets procedural control through hybrid mesh and surface modeling.</p><p class="">This creative foundation becomes even more powerful when connected to modern AI and computer vision pipelines. Thanks to insights from board member Harish Anand, I’ve recently deepened my understanding of how AI-assisted tools can help bridge the gap between rough digital assets and refined, contextualized digital twins. It’s a critical part of my work, which sits at the intersection of robotics and the sciences. Now it is enabling continuous <em>ratcheting</em> of innovation between science and art—an idea inspired by another board member, Dr. Ramon Arrowsmith in the context of science and engineering.</p><h3><strong>LLMs as Creative Co-Designers</strong></h3><p class="">Large Language Models (LLMs) are more than chatbots—they are becoming <strong>embedded infrastructure</strong> in design and engineering workflows. I use:</p><ul data-rte-list="default"><li><p class=""><strong>Cursor IDE</strong>: A deeply integrated development environment that combines Claude and other LLMs with intelligent code context.<br></p></li><li><p class=""><strong>Claude MCP</strong>: Tools that leverage the<a href="https://modelcontextprotocol.io/introduction"> <span>Model Context Protocol (MCP)</span></a> to standardize interactions with AI systems, bridging context between modeling platforms and AI assistants.<br></p></li></ul><p class="">Together, these tools let me manage complex, context-rich modeling pipelines with greater fluency and modularity.</p><h3><strong>Navagunjara and DeepGIS-XR: Fusing 3D and 2D Worlds</strong></h3><p class="">Ongoing<strong> </strong><a href="https://earthinnovationhub.org/navagunjara_reborn"><strong>Navagunjara Reborn</strong></a> Burning Man Honoraria Art Grant Project with Richa Maheshwari (Boito) found a natural integration with <a href="https://deepgis.org" target="_blank"><strong>DeepGIS</strong></a> in an attempt to blend narrative and spatial data—<a href="https://deepgis.org/label/3d/" target="_blank">combining digital twins with architectural and cartographic layers</a>. This approach enables hybrid reporting, where immersive 3D visualizations remain traceable to 2D plans, eventually through GIS systems, with metadata-rich documentation.</p><p class="">This is not just about visualization—it's about <strong>reproducibility</strong> and <strong>traceability</strong>, which are essential whether you're building a robot armature or coordinating a mixed-media art installation.</p><h3><strong><br>Model Context Protocols: Interoperability by Design</strong></h3><p class="">A key enabler of all this is the <strong>Model Context Protocol (MCP), </strong>which was first brought to my attention this March by our Burning Man camp mayor, Christopher Filkins. It's a relatively new open protocol that allows client applications—such as Blender, FreeCAD, Cursor, and Claude Desktop—to share structured context with one another.</p><p class="">Using <a href="https://github.com/ahujasid/blender-mcp" target="_blank"><strong>Blender-MCP</strong></a> and <a href="https://github.com/neka-nat/freecad-mcp" target="_blank"><strong>FreeCAD-MCP</strong></a>, I can:</p><ul data-rte-list="default"><li><p class="">Maintain <strong>semantic fidelity</strong> from generative AI outputs to engineering models.<br></p></li><li><p class="">Enable bidirectional context updates (e.g., changes in text-based reasoning or geometry sync across clients).<br></p></li><li><p class="">Automate capture of modeling intent, assumptions, and constraints—making them visible and machine-readable.<br></p></li></ul><p class="">The <strong>Claude MCP clients</strong> showcased at<a href="https://www.claudemcp.com"> <span>claudemcp.com</span></a> are a good place to start exploring this ecosystem. Applications like <em>Cursor</em> illustrate how distributed tools can coordinate design logic, AI prompting, and multi-modal outputs through a shared protocol.</p><p class="">In closing, the convergence of sculpting tools, AI co-pilots, and interoperability standards like MCP is reshaping how we think about design and fabrication. Whether you’re working with robots, installations, or digital city models, the power to integrate vision, language, and geometry into a single contextual loop is finally within reach.<br><br><br>Jnaneshwar Das, </p><p class=""><em>Tempe, Arizona</em></p>]]></description></item><item><title>Welcome to the Earth Innovation Hub!</title><dc:creator>Jnaneshwar Das</dc:creator><pubDate>Tue, 13 May 2025 18:27:24 +0000</pubDate><link>https://earthinnovationhub.org/journal/welcome-to-earth-innovation-hub</link><guid isPermaLink="false">6736ca8aad6dbf3dff111519:68170c8f3bab4c2b72741231:68238ead7e618167eee0dc29</guid><description><![CDATA[<p class="">Earth Innovation Hub (EIH) was founded with the vision of cultivating a collaborative ecosystem where innovations in Robotics and AI could serve both Earth and Space Sciences. Our community thrives on the cross-pollination of ideas—from cutting-edge engineering to artistic storytelling. At the heart of this mission lies a commitment to openness, inclusivity, and shared progress.</p><p class="">Originally launched in January 2019 to support DREAMS Laboratory and the NSF Cyber-Physical Systems Student UAV competitions, our Slack workspace has since evolved into a robust network of 107 members across universities and institutions. In 2024, our first year as a registered nonprofit, we continued to expand our research capabilities, community outreach, and technical infrastructure.</p><p class="">Highlights of our journey span robotics field tests to digital twin simulations, from high-altitude balloon experiments to public art installations, our activities reflect a broad and bold vision for impact.</p><p class="">Thank you for supporting and contributing to the Earth Innovation Hub. Here's to shaping a more innovative and interconnected future together!<br><br>Jnaneshwar Das </p><p class=""><em>Tempe, Arizona</em></p>]]></description></item></channel></rss>